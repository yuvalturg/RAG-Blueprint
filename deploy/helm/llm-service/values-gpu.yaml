servingRuntime:
  image: quay.io/ecosystem-appeng/vllm:openai-v0.8.3
  recommendedAccelerators:
  - nvidia.com/gpu

models:
  llama-3-2-1b-instruct:
    id: meta-llama/Llama-3.2-1B-Instruct
    enabled: false
    inferenceService:
      args:
      - --enable-auto-tool-choice
      - --chat-template
      - /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --max-model-len
      - "30544"

  llama-guard-3-1b:
    id: meta-llama/Llama-Guard-3-1B
    enabled: false
    inferenceService:
      args:
      - --max-model-len
      - "14336"

  llama-3-2-3b-instruct:
    id: meta-llama/Llama-3.2-3B-Instruct
    enabled: false
    inferenceService:
      args:
      - --enable-auto-tool-choice
      - --chat-template
      - /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --max-model-len
      - "30544"

  llama-guard-3-8b:
    id: meta-llama/Llama-Guard-3-8B
    enabled: false
    inferenceService:
      args:
      - --max-model-len
      - "14336"

  llama-3-3-70b-instruct:
    id: meta-llama/Llama-3.3-70B-Instruct
    enabled: false
    storageSize: 150Gi
    inferenceService:
      resources:
        limits:
          nvidia.com/gpu: "4"
      args:
      - --tensor-parallel-size
      - "4"
      - --gpu-memory-utilization
      - "0.95"
      - --quantization
      - fp8
      - --max-num-batched-tokens
      - "4096"
      - --enable-auto-tool-choice
      - --chat-template
      - /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --swap-space
      - "32"
