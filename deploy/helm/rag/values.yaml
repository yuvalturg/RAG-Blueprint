replicaCount: 1

image:
  repository: quay.io/ecosystem-appeng/llamastack-dist-ui
  pullPolicy: IfNotPresent
  tag: 0.2.5

service:
  type: ClusterIP
  port: 8501

serviceAccount:
  create: false

livenessProbe:
  httpGet:
    path: /
    port: http

readinessProbe:
  httpGet:
    path: /
    port: http

env:
  - name: LLAMA_STACK_ENDPOINT
    value: 'http://llamastack:8321'

volumes:
  - emptyDir: {}
    name: dot-streamlit

volumeMounts:
  - mountPath: /.streamlit
    name: dot-streamlit

# Common model values for llm-service and llama-stack
# See format in https://github.com/RHEcosystemAppEng/ai-architecture-charts/blob/main/helm/llm-service/values.yaml
# For e.g., to add a new model add the following block and it will append to the list of models defined in the llm-service

# global:
#   models:
#     granite-vision-3-2-2b:
#     id: ibm-granite/granite-vision-3.2-2b
#     enabled: true
#     inferenceService:
#       resources:
#         limits:
#           nvidia.com/gpu: "1"
#       tolerations:
#        - key: "nvidia.com/gpu"
#          operator: Exists
#          effect: NoSchedule
#       args:
#       - --tensor-parallel-size
#       - "1"
#       - --max-model-len
#       - "6144"
#       - --enable-auto-tool-choice
#       - --tool-call-parser
#       - granite


global:
  models: {}

pgvector:
  secret:
    user: postgres
    password: rag_password
    dbname: rag_blueprint
    host: pgvector
    port: "5432"

minio:
  secret:
    user: minio_rag_user
    password: minio_rag_password
    host: minio
    port: "9000"

llama-stack:
  mcp-servers:
     mcp-weather:
      uri: http://rag-mcp-weather:8000/sse
